{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to Read and Process eBird Data\n",
    "- Download dataset here: https://ebird.org/science/use-ebird-data/download-ebird-data-products \n",
    "- Please read readme.txt available at: https://github.com/mila-iqia/ecosystem-embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "# from datetime import time\n",
    "from collections import OrderedDict\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All data at checklists level\n",
    "- `complete-checklists.txt` is the file which has the data of complete checklists.\n",
    "- I got this `complete-checklists.txt` file from the R-code provided by Matt (maintainer of ebird)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>STATE</th>\n",
       "      <th>LOCALITY</th>\n",
       "      <th>LOCALITY ID</th>\n",
       "      <th>LOCALITY TYPE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>OBSERVER ID</th>\n",
       "      <th>SAMPLING EVENT IDENTIFIER</th>\n",
       "      <th>GROUP IDENTIFIER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>United States</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>07866 Rockaway</td>\n",
       "      <td>L126977</td>\n",
       "      <td>PC</td>\n",
       "      <td>40.917198</td>\n",
       "      <td>-74.509209</td>\n",
       "      <td>obs17638</td>\n",
       "      <td>S84135601</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Canada</td>\n",
       "      <td>British Columbia</td>\n",
       "      <td>Trial Islands</td>\n",
       "      <td>L2747482</td>\n",
       "      <td>H</td>\n",
       "      <td>48.398617</td>\n",
       "      <td>-123.304933</td>\n",
       "      <td>obs961105</td>\n",
       "      <td>S84138959</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Canada</td>\n",
       "      <td>British Columbia</td>\n",
       "      <td>Stevens Lane</td>\n",
       "      <td>L455677</td>\n",
       "      <td>P</td>\n",
       "      <td>49.014585</td>\n",
       "      <td>-123.086266</td>\n",
       "      <td>obs133217</td>\n",
       "      <td>S84136606</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>United States</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Wallingford Steps</td>\n",
       "      <td>L5759293</td>\n",
       "      <td>H</td>\n",
       "      <td>47.647237</td>\n",
       "      <td>-122.336347</td>\n",
       "      <td>obs554444</td>\n",
       "      <td>S85200104</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>United States</td>\n",
       "      <td>New York</td>\n",
       "      <td>Durland Preserve</td>\n",
       "      <td>L446377</td>\n",
       "      <td>H</td>\n",
       "      <td>42.437996</td>\n",
       "      <td>-76.397982</td>\n",
       "      <td>obs9009</td>\n",
       "      <td>S924173</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         country             STATE           LOCALITY LOCALITY ID  \\\n",
       "0  United States        New Jersey     07866 Rockaway     L126977   \n",
       "1         Canada  British Columbia      Trial Islands    L2747482   \n",
       "2         Canada  British Columbia       Stevens Lane     L455677   \n",
       "3  United States        Washington  Wallingford Steps    L5759293   \n",
       "4  United States          New York   Durland Preserve     L446377   \n",
       "\n",
       "  LOCALITY TYPE   LATITUDE   LONGITUDE OBSERVER ID SAMPLING EVENT IDENTIFIER  \\\n",
       "0            PC  40.917198  -74.509209    obs17638                 S84135601   \n",
       "1             H  48.398617 -123.304933   obs961105                 S84138959   \n",
       "2             P  49.014585 -123.086266   obs133217                 S84136606   \n",
       "3             H  47.647237 -122.336347   obs554444                 S85200104   \n",
       "4             H  42.437996  -76.397982     obs9009                   S924173   \n",
       "\n",
       "  GROUP IDENTIFIER  \n",
       "0                   \n",
       "1                   \n",
       "2                   \n",
       "3                   \n",
       "4                   "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../complete-checklists.txt', delimiter = \"\\t\", keep_default_na=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of checklists per hotspot (sorted in descending order)\n",
    "- `n_checklists.csv` was also calculated from R code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# n_checklists is the number of complete checklist per hotspot\n",
    "df_nc_hotspot = pd.read_csv('../n_checklists.csv', keep_default_na=False)\n",
    "print(df_nc_hotspot.shape)\n",
    "\n",
    "# sort by number of complete checklists\n",
    "df_nc_hotspot_sorted = df_nc_hotspot.sort_values(by=['n'], ascending = False)\n",
    "print(type(df_nc_hotspot_sorted))\n",
    "\n",
    "# Rename header of the dataframe\n",
    "df_new = df_nc_hotspot_sorted.rename({'locality_id': 'LOCALITY ID', 'locality': 'LOCALITY'}, axis=1)\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge two dataframes such that each hotspot has:\n",
    "- 'number' of complete checklists\n",
    "- other informations like latitude longitude information etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step takes time to execute. May lead to kernel restarting\n",
    "df_checklist_count = df_new.merge(df, on ='LOCALITY ID', how='inner') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_checklist_count.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only US based hotspots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_checklist_usa = df_checklist_count.loc[df_checklist_count['country'] == 'United States']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop duplicate columns when data was merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_checklist_usa_uc = df_checklist_usa.drop('LOCALITY_y', 1) # uc in df_checklist_usa_uc: unique column\n",
    "print(df_checklist_usa_uc.shape)\n",
    "df_checklist_usa_uc.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_checklist_usa_unique = df_checklist_usa.drop_duplicates('LOCALITY ID')\n",
    "df_checklist_usa_unique = df_checklist_usa_unique.sort_values(by=['n'], ascending = False)\n",
    "print(\"Number of unique hotspots in USA\", len(df_checklist_usa_unique))\n",
    "df_checklist_usa_unique.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selectonly conttinental USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c_usa = continental USA\n",
    "\n",
    "df_c_usa = df_checklist_usa_unique[(df_checklist_usa_unique.STATE != \"Alaska\") &\n",
    "                                   (df_checklist_usa_unique.STATE != \"Hawaii\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All hotspots with their number of checklists in Continental USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usa_count = df_c_usa.sort_values(by=['n'], ascending = False)\n",
    "print(\"Number of hotspots in continental USA\", len(df_usa_count))\n",
    "df_usa_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Places with complete checklist > 50\n",
    "\n",
    "threshold = 50\n",
    "df_usa_count_threshold = df_usa_count[(df_usa_count['n'] >= threshold)]\n",
    "print(len(df_usa_count_threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dict with locaity ids and country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_usa_loc_country = df_usa_count[['LOCALITY ID_x', 'country']].values\n",
    "# dict_usa = dict(df_usa_loc_country)\n",
    "# print (\"Length of dictionary:\", len(dict_usa))\n",
    "# # dict_usa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a list of hostspot IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_loc = df_usa_count_threshold['LOCALITY ID_x'].to_list()\n",
    "print (\"Length of list of locality:\", len(list_loc))\n",
    "list_loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify if a locality exists in the list `list_loc`. \n",
    "This will be needed when we need to filter the whole data file (250 GB+) to distill only those rows which are associated with hotspot id present in `list_loc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"L348850\" in list_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read ebird data\n",
    "- File location : /miniscratch/srishtiy/ebd_relJan-2021.txt\n",
    "- This file is very large (~250 GB+) and needs to read in chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read eBIRD data file in chunks because original file is huge.\n",
    "- We want all columns for now so we are not filtering column wise\n",
    "- How chuunk works: https://stackoverflow.com/questions/25962114/how-do-i-read-a-large-csv-file-with-pandas\n",
    "- chunksize = 1000 implies 1000 rows in each chunk will be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
