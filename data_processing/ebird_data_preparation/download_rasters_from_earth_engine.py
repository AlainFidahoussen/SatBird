# -*- coding: utf-8 -*-
"""Copy of download_rasters_from_earth_engine.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CaQB5M2gh0DEts4wHi98pY4yLOigPYGe

# **Instructions** 
---
### Execution Environment 

This notebook is designed to be executed in Google Colaboratory (Colab). Running it in other environments might require modifications to the code.


### Google Earth Engine Account

This notebook utilizes Google Earth Engine. Make sure you have an active Google Earth Engine Account and access to the Google Earth Engine Console.

### Google Drive Storage
The output files will be saved to your linked Google Drive account. Please verify that you have sufficient storage space available. Remember, large datasets may consume significant storage.

---
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import geopandas as gpd
from shapely.geometry import Polygon, Point
import numpy as np
import ee
from datetime import date
from datetime import datetime
import math
import logging
import ssl
import time
# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
# %matplotlib inline

# Trigger the authentication flow.
# ee.Authenticate()

# Initialize the library.
ee.Initialize()


"""# 01 - Pre-Process Data"""

# make sure to have the file list in current working dir
data = pd.read_csv("../summer_hotspots.csv")
# /network/projects/ecosystem-embeddings/ebird_new/summer_hotspots.csv
# /home/hagerradi/.config/gcloud/application_default_credentials.json

# Extract the geometry from the lat/lon 
geometry = [Point(xy) for xy in zip(data['lon'], data['lat'])]

# Convert the data into a geo_dataframae
crs = {'init':'epsg:4326'}
geo_df = gpd.GeoDataFrame(data,
                          crs=crs,
                          geometry=geometry)

aa = geo_df["num_complete_checklists"].unique()


def get_tif_files(image_path):
    """
    Get all the .tif files in the image folder.

    Parameters
    ----------
    image_path: pathlib Path
        Directory to search for tif files
    Returns:
        A list of .tif filenames
    """
    files = []
    for dir_file in image_path.iterdir():
        if str(dir_file).endswith("tif"):
            files.append(str(dir_file.name))
    return files

def generate_buffer_meter(data, radiu, geometry='geometry', crs='epsg:4326'):
    """
    Generates a buffer around the geometries in a geopandas DataFrame.

    Parameters:
    data (GeoDataFrame or DataFrame): The geopandas dataframe or a pandas dataframe that contains geometry data.
    radiu (float): The radius of the buffer in meters.
    geometry (str, optional): The column in the dataframe that contains the geometry information. 
                              Defaults to 'geometry'.
    crs (str, optional): The Coordinate Reference System to be used. Defaults to 'epsg:4326'.

    Returns:
    GeoDataFrame: A new geopandas dataframe with the buffer applied to the geometry.
    """

    data = gpd.GeoDataFrame(data)
    data = data.to_crs(crs)
    data = data.to_crs('EPSG:6933')  # Albers Equal Area Conic projection
    data[geometry] = data[geometry].buffer(radiu, cap_style=3)
    data = data.to_crs(crs)

    return data

# Generate a buffer of 3km around the geometries in a geopandas DataFrame
data_df = generate_buffer_meter(geo_df, 2500)

"""#02 -  Loading Data from Earth Engine"""

def get_single_image(region: ee.Geometry, start_date: date, end_date: date) -> ee.Image:

    dates = ee.DateRange(date_to_string(start_date), date_to_string(end_date))

    startDate = ee.DateRange(dates).start()
    endDate = ee.DateRange(dates).end()
    imgC = ee.ImageCollection(image_collection).filterDate(startDate, endDate).filterBounds(region)

    imgC = (
        imgC.map(lambda x: x.clip(region))
        .map(lambda x: x.set("ROI", region))
        .map(computeS2CloudScore)
        .map(projectShadows)
        .map(computeQualityScore)
        .sort("CLOUDY_PIXEL_PERCENTAGE")
    )

    # has to be double to be compatible with the sentinel 1 imagery, which is in
    # float64
    cloudFree = mergeCollection(imgC).toDouble()

    return cloudFree

def get_single_image_landuse(region: ee.Geometry, start_date: date, end_date: date) -> ee.Image:

    dates = ee.DateRange(date_to_string(start_date), date_to_string(end_date))

    startDate = ee.DateRange(dates).start()
    endDate = ee.DateRange(dates).end()
    imgC = ee.ImageCollection(glcd).filterDate(startDate, endDate).filterBounds(region)

    imgC = (
        imgC.map(lambda x: x.clip(region))
    )

    output = merge_lc_collection(imgC)

    return output


def rescale(img, exp, thresholds):
    return (
        img.expression(exp, {"img": img})
        .subtract(thresholds[0])
        .divide(thresholds[1] - thresholds[0])
    )


def computeQualityScore(img):
    score = img.select(["cloudScore"]).max(img.select(["shadowScore"]))

    score = score.reproject("EPSG:4326", None, 20).reduceNeighborhood(
        reducer=ee.Reducer.mean(), kernel=ee.Kernel.square(5), optimization="boxcar"
    )

    score = score.multiply(-1)

    return img.addBands(score.rename("cloudShadowScore"))


def computeS2CloudScore(img):
    toa = img.select(
        ["B1", "B2", "B3", "B4", "B5", "B6", "B7", "B8", "B8A", "B9", "B11", "B12"]
    ).divide(10000)

    toa = toa.addBands(img.select(["QA60"]))

    # ['QA60', 'B1','B2',    'B3',    'B4',   'B5','B6','B7', 'B8','  B8A',
    #  'B9',          'B10', 'B11','B12']
    # ['QA60','cb', 'blue', 'green', 'red', 're1','re2','re3','nir', 'nir2',
    #  'waterVapor', 'cirrus','swir1', 'swir2']);

    # Compute several indicators of cloudyness and take the minimum of them.
    score = ee.Image(1)

    # Clouds are reasonably bright in the blue and cirrus bands.
    score = score.min(rescale(toa, "img.B2", [0.1, 0.5]))
    score = score.min(rescale(toa, "img.B1", [0.1, 0.3]))
    score = score.min(rescale(toa, "img.B1 + img.B11", [0.15, 0.2]))

    # Clouds are reasonably bright in all visible bands.
    score = score.min(rescale(toa, "img.B4 + img.B3 + img.B2", [0.2, 0.8]))

    # Clouds are moist
    ndmi = img.normalizedDifference(["B8", "B11"])
    score = score.min(rescale(ndmi, "img", [-0.1, 0.1]))

    # However, clouds are not snow.
    ndsi = img.normalizedDifference(["B3", "B11"])
    score = score.min(rescale(ndsi, "img", [0.8, 0.6]))

    # Clip the lower end of the score
    score = score.max(ee.Image(0.001))

    # score = score.multiply(dilated)
    score = score.reduceNeighborhood(reducer=ee.Reducer.mean(), kernel=ee.Kernel.square(5))

    return img.addBands(score.rename("cloudScore"))


def projectShadows(image):
    meanAzimuth = image.get("MEAN_SOLAR_AZIMUTH_ANGLE")
    meanZenith = image.get("MEAN_SOLAR_ZENITH_ANGLE")

    cloudMask = image.select(["cloudScore"]).gt(cloudThresh)

    # Find dark pixels
    darkPixelsImg = image.select(["B8", "B11", "B12"]).divide(10000).reduce(ee.Reducer.sum())

    ndvi = image.normalizedDifference(["B8", "B4"])
    waterMask = ndvi.lt(ndviThresh)

    darkPixels = darkPixelsImg.lt(irSumThresh)

    # Get the mask of pixels which might be shadows excluding water
    darkPixelMask = darkPixels.And(waterMask.Not())
    darkPixelMask = darkPixelMask.And(cloudMask.Not())

    # Find where cloud shadows should be based on solar geometry
    # Convert to radians
    azR = ee.Number(meanAzimuth).add(180).multiply(math.pi).divide(180.0)
    zenR = ee.Number(meanZenith).multiply(math.pi).divide(180.0)

    # Find the shadows
    def getShadows(cloudHeight):
        cloudHeight = ee.Number(cloudHeight)
        
        shadowCastedDistance = zenR.tan().multiply(cloudHeight)  # Distance shadow is cast
        x = azR.sin().multiply(shadowCastedDistance).multiply(-1)  # /X distance of shadow
        y = azR.cos().multiply(shadowCastedDistance).multiply(-1)  # Y distance of shadow
        return image.select(["cloudScore"]).displace(
            ee.Image.constant(x).addBands(ee.Image.constant(y))
        )

    shadows = ee.List(cloudHeights).map(getShadows)
    shadowMasks = ee.ImageCollection.fromImages(shadows)
    shadowMask = shadowMasks.mean()

    # Create shadow mask
    shadowMask = dilatedErossion(shadowMask.multiply(darkPixelMask))

    shadowScore = shadowMask.reduceNeighborhood(
        **{"reducer": ee.Reducer.max(), "kernel": ee.Kernel.square(1)}
    )

    image = image.addBands(shadowScore.rename(["shadowScore"]))

    return image


def dilatedErossion(score):
    # Perform opening on the cloud scores

    def erode(img, distance):
        d = (
            img.Not()
            .unmask(1)
            .fastDistanceTransform(30)
            .sqrt()
            .multiply(ee.Image.pixelArea().sqrt())
        )
        return img.updateMask(d.gt(distance))

    def dilate(img, distance):
        d = img.fastDistanceTransform(30).sqrt().multiply(ee.Image.pixelArea().sqrt())
        return d.lt(distance)

    score = score.reproject("EPSG:4326", None, 20)
    score = erode(score, erodePixels)
    score = dilate(score, dilationPixels)

    return score.reproject("EPSG:4326", None, 20)


def mergeCollection(imgC):
    filtered = imgC.qualityMosaic("cloudShadowScore")
    filtered = filtered.select(BANDS)
    return filtered

def merge_lc_collection(imgC):
    filtered = imgC.qualityMosaic("cloudShadowScore")
    filtered = filtered.select(lc_bands)
    return filtered


def date_to_string(input_date):
    if isinstance(input_date, str):
        return input_date
    else:
        assert isinstance(input_date, date)
        return input_date.strftime("%Y-%m-%d")

import os

from tqdm import tqdm

#Transforms an Image Collection with 1 band per Image into a single Image with items as bands

# currently from 3082:

scale=10 
max_pixels=1e13

#Transforms an Image Collection with 1 band per Image into a single Image with items as bands
def _append_im_band(current, previous):
    # Transforms an Image Collection with 1 band per Image into a single Image with items as bands
    # Author: Jamie Vleeshouwer

    # Rename the band
    previous = ee.Image(previous)
    current = current.select(BANDS)
    # Append it to the result (Note: only return current item on first element/iteration)
    return ee.Algorithms.If(
        ee.Algorithms.IsEqual(previous, None),
        current,
        previous.addBands(ee.Image(current)),
    )

def download_sentinel2_data(data_df_, start_date, end_date, drive_folder, beg_row = 0, end_row=3000,scale=10, max_pixels=1e13):
    """
    Downloads Sentinel 2 data for unique locations from the given DataFrame.

    Parameters:
    data_df (GeoDataFrame): A GeoDataFrame that contains 'geometry' and 'LOCALITY ID' columns.
    start_date (str): The start date for the image collection in the format 'YYYY-MM-DD'.
    end_date (str): The end date for the image collection in the format 'YYYY-MM-DD'.
    drive_folder (str): The Google Drive folder to export the images to.
    scale (int, optional): The scale to use when exporting the image. Default is 10.
    max_pixels (int or float, optional): The maximum allowed number of pixels in the exported image. Default is 1e13.
    
    Returns:
    None
    """
    # Error handling for inputs
    if data_df_.empty:
        raise ValueError("Input dataframe is empty")
    if not {'geometry', 'hotspot_id'}.issubset(data_df_.columns):
        raise ValueError("Input dataframe should contain 'geometry' and 'hotspot_id' columns")
    if not datetime.strptime(start_date, '%Y-%m-%d') < datetime.strptime(end_date, '%Y-%m-%d'):
        raise ValueError("Start date should be before end date")

    data_df = data_df_[beg_row:end_row]
    data_df.reset_index(drop=True, inplace=True)

    print(data_df.head())
    # Get the total number of rows in the DataFrame
    total_rows = len(data_df)
    print(total_rows)

    # Initialize the tqdm progress bar
    progress_bar = tqdm(total=total_rows, unit='row')

    # Iterate over each row in the DataFrame
    for index, row in data_df.iterrows():
        # Log index every 20 iterations
        print(index)
        if index % 20 == 0:
            logging.info(f"Processing index: {index}")

        # Get coordinates and create a polygon
        coords = data_df["geometry"][index].exterior.coords
        contour = list((np.asarray(coords)).flatten())
        region = create_region(contour)
        # img.select(['B2', 'B3', 'B4', 'B8', 'B8A', 'B9', 'B11', 'B12', 'TCI_R', 'TCI_B', 'TCI_G']), 
        # Get the image
        img = get_single_image(region, start_date, end_date)
        img = ee.Image(_append_im_band(img, None)).select(['B2', 'B3', 'B4', 'B8', 'B8A', 'B9', 'B11', 'B12', 'TCI_R', 'TCI_B', 'TCI_G'])
        # Create export task
        while True:
            try:
                task = ee.batch.Export.image(
                    img.clip(region),
                    # img.clip(region),
                    data_df["hotspot_id"][index],
                    {"scale": scale, "region": region, "maxPixels": max_pixels, "driveFolder": drive_folder},
                )
                task.start()
            except (ee.ee_exception.EEException, ssl.SSLEOFError):
                print(f"Retrying Index {index}")
                time.sleep(10)
                continue
            break

        progress_bar.update(1)
  
    progress_bar.close()


def create_region(contour):
    """
    Creates a polygon from a contour.

    Parameters:
    contour (list): A list of coordinates.

    Returns:
    region (ee.Geometry.Polygon): A polygon that represents the region.
    """
    return ee.Geometry.Polygon(contour)

def list_ee_tasks():
    print("Running tasks:")
    running_tasks = [task for task in ee.batch.Task.list() if task.state == ee.batch.Task.State.RUNNING]
    for task in running_tasks:
        print(task.id)

    print("Completed tasks:")
    completed_tasks = [task for task in ee.batch.Task.list() if task.state == ee.batch.Task.State.COMPLETED]
    for task in completed_tasks:
        print(task.id)

def check_ee_tasks():
    # Get a list of all tasks
    tasks = ee.batch.Task.list()

    # Filter tasks that are running
    running_tasks = [task for task in tasks if task.state == ee.batch.Task.State.RUNNING]
    running_count = len(running_tasks)

    # Filter tasks that are completed
    completed_tasks = [task for task in tasks if task.state == ee.batch.Task.State.COMPLETED]
    completed_count = len(completed_tasks)

    # Filter tasks that have failed
    failed_tasks = [task for task in tasks if task.state == ee.batch.Task.State.FAILED]
    failed_count = len(failed_tasks)

    # Log the status
    logger.info(f"Running: {running_count}, Completed: {completed_count}, Failed: {failed_count}")

"""# 03 - Finally Download the Data"""

#  These are algorithm settings for the cloud filtering algorithm
image_collection = "COPERNICUS/S2_SR"

# Ranges from 0-1.Lower value will mask more pixels out.
# Generally 0.1-0.3 works well with 0.2 being used most commonly
cloudThresh = 0.2

# Height of clouds to use to project cloud shadows
cloudHeights = [200, 10000, 250]

# Sum of IR bands to include as shadows within TDOM and the
# shadow shift method (lower number masks out less)
irSumThresh = 0.3
ndviThresh = -0.1

# Pixels to reduce cloud mask and dark shadows by to reduce inclusion
# of single-pixel comission errors
erodePixels = 1.5
dilationPixels = 3

# images with less than this manyf cloud pixels will be used with normal
# mosaicing (most recent on top)
cloudFreeKeepThresh = 3

# Bands to Download
# BANDS = ["B2", "B3", "B4", "B8", "B8A", "B9", "B10", "B11", "B12"]
BANDS = ["B2", "B3", "B4", "B8", "B8A", "B9", "B11", "B12", "TCI_R", "TCI_B", "TCI_G"]

lc_bands = ["water", "trees", "grass"]

# Define the start date, end date and the Google Drive folder name
start_date = "2022-06-01"
end_date = "2022-07-31"
# drive_folder = "/home/mila/h/hager.radi/scratch/summer_hotspots_rasters_new"
drive_folder = "summer_hotspots_rasters_new"

beg_row = 100000
end_row = -1
# Call the function
download_sentinel2_data(data_df, start_date, end_date,  drive_folder, beg_row, end_row)

