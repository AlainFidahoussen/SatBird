{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b2c51f2",
   "metadata": {},
   "source": [
    "This notebook prepares the ebutterfly data, not co-located with ebird), starts with clustering the observations, creates polygons to extract the satellite images from planetary computer, filters images that are smaller than 128x128, creates the targets by aggregating the checklists, saves final csv for the hotspots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21decd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import geopandas as gpd\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from pathlib import Path    \n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from geopy.distance import great_circle\n",
    "from shapely.geometry import MultiPoint\n",
    "from shapely.geometry import Polygon, Point\n",
    "from math import cos, radians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188a0f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/ebutterfly/Darwin/0177350-230224095556074\"\n",
    "dataset_tag = \"ebutterfly_data_v3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ca6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "buttefly_data_US = pd.read_csv(os.path.join(root_dir, \"occ_usa.csv\"))\n",
    "\n",
    "print(buttefly_data_US)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe35ee7",
   "metadata": {},
   "source": [
    "# Clustering ebutterfly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e8d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "RADIUS_EARTH = 6356.7523 \n",
    "\n",
    "coordinates = buttefly_data_US[['decimalLatitude', 'decimalLongitude']].values\n",
    "\n",
    "eps = 1/RADIUS_EARTH # Maximum distance between points to be considered part of the same cluster\n",
    "min_samples = 2  # Minimum number of points in a cluster (including the core point)\n",
    "\n",
    "db = DBSCAN(eps=eps, min_samples=min_samples, algorithm='ball_tree', metric='haversine').fit(np.radians(coordinates))\n",
    "\n",
    "cluster_labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise (-1 is noise)\n",
    "num_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "num_noise = len(set(cluster_labels)) - num_clusters\n",
    "print(\"Number of clusters:\", num_clusters)\n",
    "print(\"Number of noise:\", num_noise)\n",
    "\n",
    "clusters = pd.Series([coordinates[cluster_labels == n] for n in range(num_clusters)])\n",
    "\n",
    "# print(clusters)\n",
    "def get_centermost_point(cluster):\n",
    "    centroid = (MultiPoint(cluster).centroid.x, MultiPoint(cluster).centroid.y)\n",
    "    centermost_point = min(cluster, key=lambda point: great_circle(point, centroid).m)\n",
    "    return tuple(centermost_point)\n",
    "\n",
    "centermost_points = clusters.map(get_centermost_point)\n",
    "center_lats, center_lons = zip(*centermost_points)\n",
    "\n",
    "# save final dataframe\n",
    "butterfly_data_US_clustered = buttefly_data_US\n",
    "butterfly_data_US_clustered[\"cluster_label\"] = cluster_labels\n",
    "\n",
    "butterfly_data_US_clustered = butterfly_data_US_clustered[butterfly_data_US_clustered[\"cluster_label\"] != -1]\n",
    "print(butterfly_data_US_clustered)\n",
    "cluster_labels = cluster_labels[np.where(cluster_labels != -1)]\n",
    "\n",
    "butterfly_data_US_clustered[\"center_lat\"] = [center_lats[cl] for cl in cluster_labels]\n",
    "butterfly_data_US_clustered[\"center_lon\"] = [center_lons[cl] for cl in cluster_labels]\n",
    "butterfly_data_US_clustered[\"hotspot_id\"] = [\"L\" + str(cl) for cl in cluster_labels]\n",
    "\n",
    "butterfly_data_US_clustered.reset_index(drop=True)\n",
    "\n",
    "print(butterfly_data_US_clustered)\n",
    "butterfly_data_US_clustered.to_csv(os.path.join(root_dir, dataset_tag, \"butterfly_data_clustered.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00809efc",
   "metadata": {},
   "source": [
    "# Generate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440eb14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "center_data_df = pd.read_csv(os.path.join(root_dir, dataset_tag, \"butterfly_data_clustered.csv\"), usecols=[\"hotspot_id\", \"center_lon\", \"center_lat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44311e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "center_data_df = center_data_df.drop_duplicates().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b4de79",
   "metadata": {},
   "outputs": [],
   "source": [
    "center_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ef8c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "colors = {'train': 'b', 'test':'y', 'valid':'m'}\n",
    "\n",
    "ax.scatter(x=center_data_df['center_lon'], y=center_data_df['center_lat'], color='grey')\n",
    "ax.scatter(x=buttefly_data_US['decimalLongitude'], y=buttefly_data_US['decimalLatitude'], color='red')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "ax.set_title('Coordinates on USA Map')\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2aa24e",
   "metadata": {},
   "source": [
    "### 1. Create polygons for the lats, lons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b061e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry = [Point(xy) for xy in zip(center_data_df['center_lon'], center_data_df['center_lat'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9c9e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d625598",
   "metadata": {},
   "outputs": [],
   "source": [
    "crs = {'init':'epsg:4326'}\n",
    "\n",
    "geo_df = gpd.GeoDataFrame(center_data_df,\n",
    "                          crs=crs,\n",
    "                          geometry=geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc7ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc6214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_buffer_meter(data, radius, geometry='geometry', crs='epsg:4326', projected_crs='epsg:3857'): \n",
    "    \"\"\" Generates a buffer around the geometries in a geopandas DataFrame. \n",
    "    Parameters: \n",
    "        data (GeoDataFrame or DataFrame): The geopandas dataframe or a pandas dataframe that contains geometry data. \n",
    "        radius (float): The radius of the buffer in meters. \n",
    "        geometry (str, optional): The column in the dataframe that contains the geometry information. Defaults to 'geometry'. \n",
    "        crs (str, optional): The Coordinate Reference System of the input geometries. Defaults to 'epsg:4326'. \n",
    "        projected_crs (str, optional): The projected CRS to use for buffering. Defaults to 'epsg:3857'. \n",
    "    Returns: \n",
    "        GeoDataFrame: A new geopandas dataframe with the buffer applied to the geometry. \n",
    "    \"\"\" \n",
    "    data = gpd.GeoDataFrame(data) \n",
    "    data = data.to_crs(projected_crs)\n",
    "    data[geometry] = data[geometry].buffer(radius, cap_style=3)\n",
    "    data = data.to_crs(crs)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1addd229",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = generate_buffer_meter(geo_df, 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dfcb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1088d43d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_df[\"geometry\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613627d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.iloc[0][\"geometry\"].area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5201e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df = data_df[data_df[\"geometry\"].area == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e66707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e226b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "colors = {'train': 'b', 'test':'y', 'valid':'m'}\n",
    "\n",
    "ax.scatter(x=data_df['center_lon'], y=data_df['center_lat'], color='grey')\n",
    "ax.scatter(x=copy_df['center_lon'], y=copy_df['center_lat'], color='red')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "ax.set_title('Coordinates on USA Map')\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040664da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_df[\"geometry\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4395586",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a44463",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_csv(os.path.join(root_dir, dataset_tag, \"ebutterfly_center_polygons.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03af1204",
   "metadata": {},
   "source": [
    "### 2. use the polygons file to extract satellite images from planetary compute, using the script (data_processing/ebutterfly_data_preparation/download_rasters_from_planetary_computer.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c6e29c",
   "metadata": {},
   "source": [
    "### 3. Filter satellite images and save final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5255161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "\n",
    "file_list = glob.glob(os.path.join(root_dir, dataset_tag, \"raw_images/*\"))\n",
    "\n",
    "# Select 8 random files from the list\n",
    "random_files = random.sample(file_list, 8)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "\n",
    "for i, file_path in enumerate(random_files):\n",
    "    with rio.open(file_path) as f:\n",
    "        r = f.read(3)\n",
    "        g = f.read(2)\n",
    "        b = f.read(1)\n",
    "    \n",
    "    # Create a composite image from RGB channels\n",
    "#     print(composite.shape)\n",
    "    composite = np.stack((r, g, b), axis=-1)\n",
    "    print(composite.shape)\n",
    "    \n",
    "    # Clip and normalize the values\n",
    "    normalized_composite = np.clip((composite / 10000), 0, 1)\n",
    "    \n",
    "    # Get the title from the file name\n",
    "    title = file_path.split(\"/\")[-1]\n",
    "    \n",
    "    # Plot the image in the corresponding subplot\n",
    "    ax = axes[i // 4, i % 4]\n",
    "    ax.imshow(normalized_composite)\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "# Adjust spacing and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9f1c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude images less than 128x128\n",
    "import shutil\n",
    "from pathlib import Path    \n",
    "\n",
    "dst = os.path.join(root_dir, dataset_tag, \"images\")\n",
    "file_list = glob.glob(os.path.join(root_dir, dataset_tag, \"raw_images/*\"))\n",
    "\n",
    "for i, file_path in enumerate(file_list):\n",
    "    with rio.open(file_path) as f:\n",
    "        r = f.read(3)\n",
    "        g = f.read(2)\n",
    "        b = f.read(1)\n",
    "    composite = np.stack((r, g, b), axis=-1)\n",
    "    if composite.shape[0] >= 128 and composite.shape[1] >= 128:\n",
    "        shutil.copy(file_path, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff315a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_hotspots = []\n",
    "file_list = glob.glob(os.path.join(root_dir, dataset_tag, \"images/*\"))\n",
    "for i, file_path in enumerate(file_list):\n",
    "    final_hotspots.append(str(Path(file_path).name.split(\".\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf146b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_hotspots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ba28d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_hotspots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99b8d43",
   "metadata": {},
   "source": [
    "# Create final csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae0500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "butterfly_df = pd.read_csv(os.path.join(root_dir, dataset_tag, \"butterfly_data_clustered.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa58df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "butterfly_df = butterfly_df[butterfly_df['hotspot_id'].isin(final_hotspots)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a798073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_butterfly_data = butterfly_df.groupby(['hotspot_id'])\n",
    "group_sizes = grouped_butterfly_data.size()\n",
    "print(group_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a93a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save species list of all unique species\n",
    "\n",
    "species_list = butterfly_df[\"species\"].unique().tolist()\n",
    "print(species_list)\n",
    "print(len(species_list))\n",
    "\n",
    "species_df = butterfly_df['species'].value_counts()\n",
    "\n",
    "species_df = species_df.reset_index()\n",
    "species_df.columns = ['species', 'frequency']\n",
    "\n",
    "species_df.to_csv(os.path.join(root_dir, dataset_tag, 'species_list.csv'), index=False)\n",
    "\n",
    "print(species_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ddcc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create targets by aggregating checklists\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "for group_name, group_data in tqdm(grouped_butterfly_data):\n",
    "    print(group_name, group_data['eventID'], group_data['occurrenceID'],\n",
    "          group_data['taxonID'], group_data['decimalLatitude'], group_data['decimalLongitude'])\n",
    "    target = {}\n",
    "    checklist_ = np.zeros(len(species_list))\n",
    "    for sp in group_data[\"species\"]:\n",
    "        checklist_[species_list.index(sp)] += 1\n",
    "    target['num_complete_checklists'] = len(group_data['eventID'].unique())\n",
    "    checklist_ = checklist_ / target['num_complete_checklists']\n",
    "    target['probs'] = checklist_.tolist()\n",
    "    target['hotspot_id'] = group_name\n",
    "\n",
    "    with open(os.path.join(root_dir, dataset_tag, 'butterfly_targets', str(group_name) + '.json'), 'w') as fp:\n",
    "        json.dump(target, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bcdcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final csv\n",
    "# columns: hotspot_name, lon, lat, number_of_observations, number_of_unique_checklists, number_of_unique_species, env variables\n",
    "hotspot_ids = []\n",
    "lats, lons = [], []\n",
    "number_of_butterfly_obs = []\n",
    "number_of_unique_checklists = []\n",
    "number_of_different_species = []\n",
    "states = []\n",
    "\n",
    "bio_env_column_names = ['bio_1', 'bio_2', 'bio_3', 'bio_4', 'bio_5',\n",
    "       'bio_6', 'bio_7', 'bio_8', 'bio_9', 'bio_10', 'bio_11', 'bio_12',\n",
    "       'bio_13', 'bio_14', 'bio_15', 'bio_16', 'bio_17', 'bio_18', 'bio_19']\n",
    "ped_env_column_names = ['bdticm', 'bldfie', 'cecsol', 'clyppt', 'orcdrc', 'phihox', 'sltppt', 'sndppt']\n",
    "location_info = ['county', 'stateProvince', 'countryCode']\n",
    "\n",
    "for group_name, group_data in tqdm(grouped_butterfly_data):\n",
    "    hotspot_ids.append(\"L\" + str(int(group_name)))\n",
    "    lats.append(group_data['center_lat'].iloc[0])\n",
    "    lons.append(group_data['center_lon'].iloc[0])\n",
    "    states.append(group_data['stateProvince'].iloc[0])\n",
    "    number_of_butterfly_obs.append(len(group_data['occurrenceID']))\n",
    "    number_of_unique_checklists.append(len(group_data['eventID'].unique()))\n",
    "    number_of_different_species.append(len(group_data['species'].unique()))\n",
    "\n",
    "final_data_frame = pd.DataFrame({'hotspot_id': hotspot_ids,\n",
    "                                 'lat': lats,\n",
    "                                 'lon': lons,\n",
    "                                 'county_code': states,\n",
    "                                 'ebutterfly_occurances': number_of_butterfly_obs,\n",
    "                                 'num_checklists': number_of_unique_checklists,\n",
    "                                 'num_species': number_of_different_species})\n",
    "\n",
    "print(final_data_frame)\n",
    "\n",
    "final_data_frame.to_csv(os.path.join(root_dir, dataset_tag, 'butterfly_hotspots.csv') , index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a1840b",
   "metadata": {},
   "source": [
    "### split data using DBSCAN (script: make_splits_by_distance.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49af3de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "butterfly_data_with_split = pd.read_csv(os.path.join(root_dir, dataset_tag, \"butterfly_hotspots_with_splits.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a40ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "butterfly_data_with_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b3d279",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_butterfly_data = butterfly_data_with_split.groupby(['split'], as_index=False)\n",
    "\n",
    "for group_name, group_data in tqdm(grouped_butterfly_data):\n",
    "    print(group_name)\n",
    "    print(group_data[\"ebutterfly_occurances\"].max())\n",
    "    group_data.to_csv(os.path.join(root_dir, dataset_tag, \"butterfly_hotspots_\" + str(group_name) + \".csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e070619c",
   "metadata": {},
   "source": [
    "### Visualize map after splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6afe36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path().resolve().parent))\n",
    "sys.path.append(str(Path().resolve().parent.parent))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from shapely.geometry import Point\n",
    "\n",
    "path = os.path.join(root_dir, dataset_tag, \"butterfly_hotspots_with_splits.csv\")\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "df = df.drop_duplicates(\"hotspot_id\")\n",
    "\n",
    "geoDatav = gpd.read_file('https://raw.githubusercontent.com/holtzy/The-Python-Graph-Gallery/master/static/data/US-counties.geojson')\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(df['lon'], df['lat'])]\n",
    "gdf = gpd.GeoDataFrame(df, geometry=geometry)   \n",
    "\n",
    "\n",
    "train = pd.read_csv(os.path.join(root_dir, dataset_tag, \"butterfly_hotspots_train.csv\"))\n",
    "val = pd.read_csv(os.path.join(root_dir, dataset_tag, \"butterfly_hotspots_valid.csv\"))\n",
    "test = pd.read_csv(os.path.join(root_dir, dataset_tag, \"butterfly_hotspots_test.csv\"))\n",
    "gdf[\"split\"] = \"\"\n",
    "\n",
    "idx = gdf[gdf[\"hotspot_id\"].isin(list(train[\"hotspot_id\"]))].index\n",
    "gdf.loc[idx,\"split\"] = \"train\"\n",
    "\n",
    "idx = gdf[gdf[\"hotspot_id\"].isin(list(val[\"hotspot_id\"]))].index\n",
    "gdf.loc[idx,\"split\"] = \"val\"\n",
    "idx = gdf[gdf[\"hotspot_id\"].isin(list(test[\"hotspot_id\"]))].index\n",
    "gdf.loc[idx,\"split\"] = \"test\"\n",
    "\n",
    "ig, ax = plt.subplots(figsize =(15,10))\n",
    "#train_gdf.drop_duplicates([\"geometry\"]).boundary.plot(ax = ax, alpha = 0.4, edgecolor = \"gray\")\n",
    "geoDatav[~geoDatav[\"STATE\"].isin([\"02\", \"15\"])].boundary.plot(ax=ax, alpha = 0.1, edgecolor = \"gray\" )\n",
    "gdf[gdf[\"split\"]==\"train\"].plot(ax=ax,marker='o', color='mediumslateblue', markersize=1, label = \"train\")\n",
    "gdf[gdf[\"split\"]==\"val\"].plot(ax=ax, marker='o', color='lightseagreen', markersize=1, label = \"val\")\n",
    "gdf[gdf[\"split\"]==\"test\"].plot(ax=ax, marker='o', color='lightsalmon', markersize=1, label = \"test\")\n",
    "\n",
    "plt.legend(fontsize=16, markerscale=5,loc='lower right',  bbox_to_anchor=(0.92, 0.25))\n",
    "plt.title(\"butterfly Hotspots\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab83f14",
   "metadata": {},
   "source": [
    "Final files saved:\n",
    "[('valid', 1164), ('test', 1166), ('train', 5436)]\n",
    "- butterfly_hotspots.csv\n",
    "- butterfly_hotspots_train.csv\n",
    "- butterfly_hotspots_valid.csv\n",
    "- butterfly_hotspots_test.csv\n",
    "- species_list.csv\n",
    "- targets/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
